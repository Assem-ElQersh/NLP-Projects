{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 5: Topic Modeling on News Articles (Unsupervised)\n",
        "\n",
        "## Overview\n",
        "This notebook implements comprehensive topic modeling using both LDA (Latent Dirichlet Allocation) and NMF (Non-negative Matrix Factorization) to discover hidden topics in news articles. We'll analyze the **BBC News Dataset** to identify and visualize significant topic patterns.\n",
        "\n",
        "## Learning Objectives\n",
        "- Understand topic modeling concepts and algorithms\n",
        "- Implement LDA and NMF for topic discovery\n",
        "- Compare different topic modeling approaches\n",
        "- Create interactive topic visualizations using pyLDAvis\n",
        "- Analyze topic-word distributions and coherence\n",
        "- Apply topic modeling to real-world news analysis\n",
        "\n",
        "## Dataset\n",
        "We'll use the BBC News Dataset with:\n",
        "- **Source**: BBC RSS feeds collection\n",
        "- **Features**: title, pubDate, guid, link, description\n",
        "- **Content**: Real-time news articles from multiple categories\n",
        "- **Size**: Large-scale news corpus for robust topic discovery\n",
        "\n",
        "## Topic Modeling Algorithms\n",
        "- **LDA (Latent Dirichlet Allocation)**: Probabilistic topic modeling\n",
        "- **NMF (Non-negative Matrix Factorization)**: Matrix factorization approach\n",
        "- **Comparison**: Performance and interpretability analysis\n",
        "\n",
        "## Pipeline Overview\n",
        "1. **Data Loading & Exploration**\n",
        "2. **Text Preprocessing** (cleaning, tokenization, filtering)\n",
        "3. **Feature Extraction** (TF-IDF, document-term matrices)\n",
        "4. **LDA Topic Modeling** (training, optimization, evaluation)\n",
        "5. **NMF Topic Modeling** (alternative approach comparison)\n",
        "6. **Topic Visualization** (pyLDAvis, word clouds, distributions)\n",
        "7. **Topic Analysis** (coherence, perplexity, interpretability)\n",
        "8. **Real-world Applications** (document classification, trend analysis)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Matplotlib available!\n",
            "✅ Gensim available for LDA modeling!\n",
            "✅ pyLDAvis available for interactive visualization!\n",
            "✅ WordCloud available!\n",
            "✅ NLTK available!\n",
            "❌ Custom TextPreprocessor not available - will use basic preprocessing\n",
            "✅ Core libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try importing matplotlib with error handling\n",
        "try:\n",
        "    import matplotlib\n",
        "    matplotlib.use('Agg')  # Use non-interactive backend\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    MATPLOTLIB_AVAILABLE = True\n",
        "    print(\"✅ Matplotlib available!\")\n",
        "except Exception as e:\n",
        "    MATPLOTLIB_AVAILABLE = False\n",
        "    print(f\"❌ Matplotlib error: {e}\")\n",
        "    print(\"Continuing without plotting capabilities...\")\n",
        "\n",
        "# Topic modeling libraries\n",
        "try:\n",
        "    import gensim\n",
        "    from gensim import corpora, models\n",
        "    from gensim.models import LdaModel, CoherenceModel\n",
        "    from gensim.utils import simple_preprocess\n",
        "    GENSIM_AVAILABLE = True\n",
        "    print(\"✅ Gensim available for LDA modeling!\")\n",
        "except ImportError:\n",
        "    GENSIM_AVAILABLE = False\n",
        "    print(\"❌ Gensim not available. Install with: pip install gensim\")\n",
        "\n",
        "try:\n",
        "    import pyLDAvis\n",
        "    if GENSIM_AVAILABLE:\n",
        "        import pyLDAvis.gensim_models as pyLDAvis_gensim\n",
        "    PYLDAVIS_AVAILABLE = True\n",
        "    print(\"✅ pyLDAvis available for interactive visualization!\")\n",
        "except ImportError:\n",
        "    PYLDAVIS_AVAILABLE = False\n",
        "    print(\"❌ pyLDAvis not available. Install with: pip install pyldavis\")\n",
        "\n",
        "# Standard ML libraries\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "try:\n",
        "    from wordcloud import WordCloud\n",
        "    WORDCLOUD_AVAILABLE = True\n",
        "    print(\"✅ WordCloud available!\")\n",
        "except ImportError:\n",
        "    WORDCLOUD_AVAILABLE = False\n",
        "    print(\"❌ WordCloud not available\")\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "import string\n",
        "\n",
        "# NLTK with error handling\n",
        "try:\n",
        "    import nltk\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    \n",
        "    # Download NLTK data quietly\n",
        "    try:\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "        NLTK_AVAILABLE = True\n",
        "        print(\"✅ NLTK available!\")\n",
        "    except:\n",
        "        NLTK_AVAILABLE = False\n",
        "        print(\"❌ NLTK data download failed\")\n",
        "except ImportError:\n",
        "    NLTK_AVAILABLE = False\n",
        "    print(\"❌ NLTK not available\")\n",
        "\n",
        "# Import our custom utilities\n",
        "import sys\n",
        "sys.path.append('./utils')\n",
        "\n",
        "try:\n",
        "    from preprocessing import TextPreprocessor\n",
        "    PREPROCESSOR_AVAILABLE = True\n",
        "    print(\"✅ Custom TextPreprocessor available!\")\n",
        "except ImportError:\n",
        "    PREPROCESSOR_AVAILABLE = False\n",
        "    print(\"❌ Custom TextPreprocessor not available - will use basic preprocessing\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"✅ Core libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Exploration\n",
        "\n",
        "We'll load the BBC News dataset and explore its structure for topic modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading BBC News Dataset...\n",
            "✅ Successfully loaded BBC News dataset!\n",
            "Dataset shape: (42115, 5)\n",
            "Columns: ['title', 'pubDate', 'guid', 'link', 'description']\n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 42115 entries, 0 to 42114\n",
            "Data columns (total 5 columns):\n",
            " #   Column       Non-Null Count  Dtype \n",
            "---  ------       --------------  ----- \n",
            " 0   title        42115 non-null  object\n",
            " 1   pubDate      42115 non-null  object\n",
            " 2   guid         42115 non-null  object\n",
            " 3   link         42115 non-null  object\n",
            " 4   description  42115 non-null  object\n",
            "dtypes: object(5)\n",
            "memory usage: 1.6+ MB\n",
            "None\n",
            "\n",
            "First few rows:\n",
            "                                               title  \\\n",
            "0  Ukraine: Angry Zelensky vows to punish Russian...   \n",
            "1  War in Ukraine: Taking cover in a town under a...   \n",
            "2         Ukraine war 'catastrophic for global food'   \n",
            "3  Manchester Arena bombing: Saffie Roussos's par...   \n",
            "4  Ukraine conflict: Oil price soars to highest l...   \n",
            "\n",
            "                         pubDate  \\\n",
            "0  Mon, 07 Mar 2022 08:01:56 GMT   \n",
            "1  Sun, 06 Mar 2022 22:49:58 GMT   \n",
            "2  Mon, 07 Mar 2022 00:14:42 GMT   \n",
            "3  Mon, 07 Mar 2022 00:05:40 GMT   \n",
            "4  Mon, 07 Mar 2022 08:15:53 GMT   \n",
            "\n",
            "                                               guid  \\\n",
            "0  https://www.bbc.co.uk/news/world-europe-60638042   \n",
            "1  https://www.bbc.co.uk/news/world-europe-60641873   \n",
            "2      https://www.bbc.co.uk/news/business-60623941   \n",
            "3            https://www.bbc.co.uk/news/uk-60579079   \n",
            "4      https://www.bbc.co.uk/news/business-60642786   \n",
            "\n",
            "                                                link  \\\n",
            "0  https://www.bbc.co.uk/news/world-europe-606380...   \n",
            "1  https://www.bbc.co.uk/news/world-europe-606418...   \n",
            "2  https://www.bbc.co.uk/news/business-60623941?a...   \n",
            "3  https://www.bbc.co.uk/news/uk-60579079?at_medi...   \n",
            "4  https://www.bbc.co.uk/news/business-60642786?a...   \n",
            "\n",
            "                                         description  \n",
            "0  The Ukrainian president says the country will ...  \n",
            "1  Jeremy Bowen was on the frontline in Irpin, as...  \n",
            "2  One of the world's biggest fertiliser firms sa...  \n",
            "3  The parents of the Manchester Arena bombing's ...  \n",
            "4  Consumers are feeling the impact of higher ene...  \n",
            "\n",
            "================================================================================\n",
            "TEXT ANALYSIS\n",
            "================================================================================\n",
            "Average text length: 164.55 characters\n",
            "Average word count: 27.21 words\n",
            "Median text length: 158.00 characters\n",
            "Max text length: 332 characters\n",
            "Min text length: 34 characters\n",
            "\n",
            "Publication Date Range:\n",
            "From: 2013-08-30 01:01:55\n",
            "To: 2024-12-04 00:05:52\n",
            "Time span: 4113 days\n",
            "\n",
            "================================================================================\n",
            "SAMPLE ARTICLES\n",
            "================================================================================\n",
            "\n",
            "Article 1:\n",
            "Title: Ukraine: Angry Zelensky vows to punish Russian atrocities\n",
            "Date: 2022-03-07 08:01:56\n",
            "Description: The Ukrainian president says the country will not forgive or forget those who murder its civilians....\n",
            "------------------------------------------------------------\n",
            "\n",
            "Article 2:\n",
            "Title: War in Ukraine: Taking cover in a town under attack\n",
            "Date: 2022-03-06 22:49:58\n",
            "Description: Jeremy Bowen was on the frontline in Irpin, as residents came under Russian fire while trying to flee....\n",
            "------------------------------------------------------------\n",
            "\n",
            "Article 3:\n",
            "Title: Ukraine war 'catastrophic for global food'\n",
            "Date: 2022-03-07 00:14:42\n",
            "Description: One of the world's biggest fertiliser firms says the conflict could deliver a shock to food supplies....\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Load the BBC News dataset\n",
        "print(\"Loading BBC News Dataset...\")\n",
        "\n",
        "df = pd.read_csv('../BBC News Dataset/bbc_news.csv')\n",
        "\n",
        "print(\"✅ Successfully loaded BBC News dataset!\")\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Basic dataset information\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Combine title and description for comprehensive text analysis\n",
        "df['full_text'] = df['title'].astype(str) + ' ' + df['description'].astype(str)\n",
        "\n",
        "# Basic text statistics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TEXT ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df['text_length'] = df['full_text'].str.len()\n",
        "df['word_count'] = df['full_text'].str.split().str.len()\n",
        "\n",
        "print(f\"Average text length: {df['text_length'].mean():.2f} characters\")\n",
        "print(f\"Average word count: {df['word_count'].mean():.2f} words\")\n",
        "print(f\"Median text length: {df['text_length'].median():.2f} characters\")\n",
        "print(f\"Max text length: {df['text_length'].max()} characters\")\n",
        "print(f\"Min text length: {df['text_length'].min()} characters\")\n",
        "\n",
        "# Publication date analysis\n",
        "print(f\"\\nPublication Date Range:\")\n",
        "df['pubDate'] = pd.to_datetime(df['pubDate'])\n",
        "print(f\"From: {df['pubDate'].min()}\")\n",
        "print(f\"To: {df['pubDate'].max()}\")\n",
        "print(f\"Time span: {(df['pubDate'].max() - df['pubDate'].min()).days} days\")\n",
        "\n",
        "# Show sample articles\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAMPLE ARTICLES\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nArticle {i+1}:\")\n",
        "    print(f\"Title: {df.iloc[i]['title']}\")\n",
        "    print(f\"Date: {df.iloc[i]['pubDate']}\")\n",
        "    print(f\"Description: {df.iloc[i]['description'][:200]}...\")\n",
        "    print(\"-\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Text preprocessing functions ready!\n"
          ]
        }
      ],
      "source": [
        "# Basic text preprocessing function (backup if custom preprocessor not available)\n",
        "def basic_text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    Basic text preprocessing for topic modeling\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    \n",
        "    # Remove extra whitespaces\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    return text\n",
        "\n",
        "def get_stopwords():\n",
        "    \"\"\"Get stopwords with fallback options\"\"\"\n",
        "    if NLTK_AVAILABLE:\n",
        "        try:\n",
        "            return set(stopwords.words('english'))\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    # Fallback stopwords list\n",
        "    return {\n",
        "        'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
        "        'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
        "        'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
        "        'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
        "        'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
        "        'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
        "        'while', 'of', 'at', 'by', 'for', 'with', 'through', 'during', 'before', 'after',\n",
        "        'above', 'below', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again',\n",
        "        'further', 'then', 'once'\n",
        "    }\n",
        "\n",
        "# Enhanced text preprocessing\n",
        "def preprocess_for_topic_modeling(text):\n",
        "    \"\"\"\n",
        "    Comprehensive text preprocessing for topic modeling\n",
        "    \"\"\"\n",
        "    if PREPROCESSOR_AVAILABLE:\n",
        "        # Use custom preprocessor if available\n",
        "        preprocessor = TextPreprocessor(\n",
        "            remove_html=True,\n",
        "            expand_contractions=False,  # Skip for efficiency\n",
        "            to_lowercase=True,\n",
        "            remove_punctuation=True,\n",
        "            remove_numbers=True,\n",
        "            remove_stopwords=True,\n",
        "            lemmatize=True,\n",
        "            min_length=3\n",
        "        )\n",
        "        return preprocessor.preprocess_text(text)\n",
        "    else:\n",
        "        # Use basic preprocessing\n",
        "        processed = basic_text_preprocessing(text)\n",
        "        \n",
        "        # Remove stopwords manually\n",
        "        stop_words = get_stopwords()\n",
        "        words = processed.split()\n",
        "        words = [word for word in words if word not in stop_words and len(word) >= 3]\n",
        "        \n",
        "        return ' '.join(words)\n",
        "\n",
        "print(\"✅ Text preprocessing functions ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading BBC News Dataset...\n",
            "✅ Successfully loaded BBC News dataset!\n",
            "Dataset shape: (42115, 5)\n",
            "Columns: ['title', 'pubDate', 'guid', 'link', 'description']\n",
            "\n",
            "Missing values:\n",
            "title          0\n",
            "pubDate        0\n",
            "guid           0\n",
            "link           0\n",
            "description    0\n",
            "dtype: int64\n",
            "\n",
            "Using sample of 5000 articles for topic modeling\n",
            "After filtering short articles: 5000 articles\n",
            "\n",
            "Text Statistics:\n",
            "Average text length: 163.80 characters\n",
            "Average word count: 27.08 words\n",
            "Median text length: 158.00 characters\n",
            "\n",
            "============================================================\n",
            "SAMPLE ARTICLES\n",
            "============================================================\n",
            "\n",
            "Article 1:\n",
            "Title: US's only Palestinian-American Congresswoman censured over comments\n",
            "Description: Michigan Democrat defends pro-Palestinian \"river to sea\" chant as an \"aspirational call for freedom\"....\n",
            "--------------------------------------------------\n",
            "\n",
            "Article 2:\n",
            "Title: Murdered driver's family demand help for couriers\n",
            "Description: Mark Lang was killed by a man who was stealing his parcel delivery van....\n",
            "--------------------------------------------------\n",
            "\n",
            "Article 3:\n",
            "Title: Cleared pony owner criticises 'trial by social media'\n",
            "Description: Sarah Moulds also criticises the RSPCA, saying it was \"pressured\" to prosecute by \"online bullies\"....\n",
            "--------------------------------------------------\n",
            "✅ Data exploration completed!\n"
          ]
        }
      ],
      "source": [
        "# Load and explore the BBC News dataset\n",
        "print(\"Loading BBC News Dataset...\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv('../BBC News Dataset/bbc_news.csv')\n",
        "    print(\"✅ Successfully loaded BBC News dataset!\")\n",
        "    \n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(f\"\\nMissing values:\")\n",
        "    print(df.isnull().sum())\n",
        "    \n",
        "    # Sample the dataset for efficient processing (for demonstration)\n",
        "    sample_size = min(5000, len(df))  # Use 5000 articles or all if less\n",
        "    df_sample = df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
        "    \n",
        "    print(f\"\\nUsing sample of {len(df_sample)} articles for topic modeling\")\n",
        "    \n",
        "    # Combine title and description for comprehensive text analysis\n",
        "    df_sample['full_text'] = df_sample['title'].astype(str) + ' ' + df_sample['description'].astype(str)\n",
        "    \n",
        "    # Remove very short articles\n",
        "    df_sample = df_sample[df_sample['full_text'].str.len() > 50].reset_index(drop=True)\n",
        "    \n",
        "    print(f\"After filtering short articles: {len(df_sample)} articles\")\n",
        "    \n",
        "    # Basic text statistics\n",
        "    df_sample['text_length'] = df_sample['full_text'].str.len()\n",
        "    df_sample['word_count'] = df_sample['full_text'].str.split().str.len()\n",
        "    \n",
        "    print(f\"\\nText Statistics:\")\n",
        "    print(f\"Average text length: {df_sample['text_length'].mean():.2f} characters\")\n",
        "    print(f\"Average word count: {df_sample['word_count'].mean():.2f} words\")\n",
        "    print(f\"Median text length: {df_sample['text_length'].median():.2f} characters\")\n",
        "    \n",
        "    # Show sample articles\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"SAMPLE ARTICLES\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    for i in range(min(3, len(df_sample))):\n",
        "        print(f\"\\nArticle {i+1}:\")\n",
        "        print(f\"Title: {df_sample.iloc[i]['title']}\")\n",
        "        print(f\"Description: {df_sample.iloc[i]['description'][:150]}...\")\n",
        "        print(\"-\" * 50)\n",
        "    \n",
        "    print(\"✅ Data exploration completed!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading dataset: {e}\")\n",
        "    print(\"Please check the dataset path and format\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Text Preprocessing for Topic Modeling\n",
        "\n",
        "Let's preprocess the text data for optimal topic modeling performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing text data for topic modeling...\n",
            "Applying text preprocessing...\n",
            "After preprocessing: 5000 articles remain\n",
            "\n",
            "================================================================================\n",
            "PREPROCESSING EXAMPLES\n",
            "================================================================================\n",
            "\n",
            "Example 1:\n",
            "Original: US's only Palestinian-American Congresswoman censured over comments Michigan Democrat defends pro-Palestinian \"river to sea\" chant as an \"aspirational call for freedom\"....\n",
            "Processed: uss palestinianamerican congresswoman censured comments michigan democrat defends propalestinian river sea chant aspirational call freedom...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Example 2:\n",
            "Original: Murdered driver's family demand help for couriers Mark Lang was killed by a man who was stealing his parcel delivery van....\n",
            "Processed: murdered drivers family demand help couriers mark lang killed man stealing parcel delivery van...\n",
            "------------------------------------------------------------\n",
            "\n",
            "Vocabulary Statistics:\n",
            "Total words: 86,884\n",
            "Unique words: 15,335\n",
            "Average words per document: 17.38\n",
            "\n",
            "Most frequent words:\n",
            "  says: 799\n",
            "  world: 434\n",
            "  england: 419\n",
            "  new: 378\n",
            "  cup: 366\n",
            "  first: 317\n",
            "  people: 306\n",
            "  bbc: 306\n",
            "  ukraine: 305\n",
            "  win: 266\n",
            "  say: 242\n",
            "  police: 242\n",
            "  league: 227\n",
            "  years: 216\n",
            "  war: 214\n",
            "✅ Text preprocessing completed!\n"
          ]
        }
      ],
      "source": [
        "# Text preprocessing for topic modeling\n",
        "print(\"Preprocessing text data for topic modeling...\")\n",
        "\n",
        "if 'df_sample' in locals():\n",
        "    # Apply preprocessing to the full text\n",
        "    print(\"Applying text preprocessing...\")\n",
        "    df_sample['processed_text'] = df_sample['full_text'].apply(preprocess_for_topic_modeling)\n",
        "    \n",
        "    # Remove empty processed texts\n",
        "    df_sample = df_sample[df_sample['processed_text'].str.len() > 0].reset_index(drop=True)\n",
        "    \n",
        "    print(f\"After preprocessing: {len(df_sample)} articles remain\")\n",
        "    \n",
        "    # Show preprocessing examples\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(\"PREPROCESSING EXAMPLES\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    for i in range(min(2, len(df_sample))):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Original: {df_sample.iloc[i]['full_text'][:200]}...\")\n",
        "        print(f\"Processed: {df_sample.iloc[i]['processed_text'][:200]}...\")\n",
        "        print(\"-\" * 60)\n",
        "    \n",
        "    # Basic word frequency analysis\n",
        "    all_words = ' '.join(df_sample['processed_text']).split()\n",
        "    word_freq = Counter(all_words)\n",
        "    \n",
        "    print(f\"\\nVocabulary Statistics:\")\n",
        "    print(f\"Total words: {len(all_words):,}\")\n",
        "    print(f\"Unique words: {len(word_freq):,}\")\n",
        "    print(f\"Average words per document: {len(all_words)/len(df_sample):.2f}\")\n",
        "    \n",
        "    print(f\"\\nMost frequent words:\")\n",
        "    for word, count in word_freq.most_common(15):\n",
        "        print(f\"  {word}: {count}\")\n",
        "    \n",
        "    # Prepare documents for topic modeling\n",
        "    documents = df_sample['processed_text'].tolist()\n",
        "    \n",
        "    print(\"✅ Text preprocessing completed!\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ No data available for preprocessing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. LDA Topic Modeling with Scikit-learn\n",
        "\n",
        "Let's implement LDA (Latent Dirichlet Allocation) using scikit-learn for reliable topic discovery.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Implementing LDA Topic Modeling...\n",
            "Creating TF-IDF vectors...\n",
            "TF-IDF matrix shape: (5000, 1000)\n",
            "Vocabulary size: 1000\n",
            "\n",
            "Training LDA model...\n",
            "✅ LDA model trained with 8 topics\n",
            "Model perplexity: 2406.33\n",
            "Log likelihood: -97812.68\n",
            "\n",
            "================================================================================\n",
            "LDA TOPICS DISCOVERED\n",
            "================================================================================\n",
            "\n",
            "Topic 1:\n",
            "  Top words: women(25.483), saying(23.362), leave(22.653), media(17.274), story(17.169), change(17.062), climate(16.734), according(16.304), tell(16.174), office(15.939)\n",
            "\n",
            "Topic 2:\n",
            "  Top words: trump(24.690), found(17.946), prison(14.682), court(14.494), missing(11.771), body(11.476), harris(11.020), house(10.991), lost(10.987), donald(10.307)\n",
            "\n",
            "Topic 3:\n",
            "  Top words: covid(26.297), staff(22.873), health(22.415), living(22.317), energy(21.950), nhs(20.007), pay(19.349), care(19.209), find(19.071), rules(18.958)\n",
            "\n",
            "Topic 4:\n",
            "  Top words: ukraine(67.697), war(51.131), russian(38.454), russia(33.936), israel(28.227), gaza(26.498), ukraine war(20.816), attack(20.295), killed(16.534), china(15.776)\n",
            "\n",
            "Topic 5:\n",
            "  Top words: police(48.324), family(28.116), man(28.094), dies(26.397), death(23.366), died(22.265), king(21.663), life(21.307), school(20.365), accused(20.025)\n",
            "\n",
            "Topic 6:\n",
            "  Top words: world(62.309), cup(60.541), england(56.536), win(46.032), world cup(45.819), league(40.104), final(26.947), victory(24.879), euro(24.242), womens(24.003)\n",
            "\n",
            "Topic 7:\n",
            "  Top words: election(38.358), papers(30.822), vote(27.444), minister(22.186), sunak(18.945), party(18.724), elections(18.232), rishi(17.707), mps(16.744), council(16.411)\n",
            "\n",
            "Topic 8:\n",
            "  Top words: week(20.597), days(14.916), music(13.585), images(13.381), past(12.762), around(12.447), storm(12.393), sea(12.126), pictures(11.959), north(11.718)\n",
            "\n",
            "============================================================\n",
            "DOCUMENT-TOPIC ANALYSIS\n",
            "============================================================\n",
            "Topic distribution across documents:\n",
            "  Topic 1: 357 documents (7.1%)\n",
            "  Topic 2: 253 documents (5.1%)\n",
            "  Topic 3: 850 documents (17.0%)\n",
            "  Topic 4: 610 documents (12.2%)\n",
            "  Topic 5: 923 documents (18.5%)\n",
            "  Topic 6: 1186 documents (23.7%)\n",
            "  Topic 7: 525 documents (10.5%)\n",
            "  Topic 8: 296 documents (5.9%)\n",
            "\n",
            "Example documents by topic:\n",
            "\n",
            "Topic 1 example:\n",
            "  Title: Cleared pony owner criticises 'trial by social media'\n",
            "  Confidence: 0.759\n",
            "  Content: Sarah Moulds also criticises the RSPCA, saying it was \"pressured\" to prosecute by \"online bullies\"....\n",
            "\n",
            "Topic 2 example:\n",
            "  Title: What does the ICJ's ruling on Israel's Rafah offensive mean?\n",
            "  Confidence: 0.300\n",
            "  Content: Last week the UN's top court delivered the latest in a series of contested rulings in a case brought by S Africa....\n",
            "\n",
            "Topic 3 example:\n",
            "  Title: Sudan conflict: Army and RSF agree deal to protect civilians\n",
            "  Confidence: 0.692\n",
            "  Content: They agree to allow safe passage for civilians and to protect relief workers but not to a ceasefire....\n",
            "✅ LDA topic modeling completed!\n"
          ]
        }
      ],
      "source": [
        "# LDA Topic Modeling with Scikit-learn\n",
        "print(\"Implementing LDA Topic Modeling...\")\n",
        "\n",
        "if 'documents' in locals() and len(documents) > 0:\n",
        "    \n",
        "    # Step 1: Create TF-IDF vectors for LDA\n",
        "    print(\"Creating TF-IDF vectors...\")\n",
        "    \n",
        "    # Custom stopwords for news articles\n",
        "    news_stopwords = get_stopwords().union({\n",
        "        'said', 'say', 'says', 'new', 'also', 'would', 'could', 'one', 'two',\n",
        "        'first', 'last', 'year', 'years', 'time', 'people', 'way', 'get',\n",
        "        'make', 'go', 'see', 'know', 'take', 'come', 'think', 'look', 'use',\n",
        "        'work', 'want', 'good', 'back', 'may', 'well', 'much', 'many',\n",
        "        'bbc', 'news', 'report', 'article'  # News-specific stopwords\n",
        "    })\n",
        "    \n",
        "    # TF-IDF Vectorization for LDA\n",
        "    tfidf_vectorizer = TfidfVectorizer(\n",
        "        max_features=1000,  # Limit vocabulary for efficiency\n",
        "        min_df=2,          # Must appear in at least 2 documents\n",
        "        max_df=0.95,       # Must appear in less than 95% of documents\n",
        "        stop_words=list(news_stopwords),\n",
        "        ngram_range=(1, 2),  # Unigrams and bigrams\n",
        "        lowercase=True\n",
        "    )\n",
        "    \n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "    \n",
        "    print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "    print(f\"Vocabulary size: {len(feature_names)}\")\n",
        "    \n",
        "    # Step 2: LDA Model Training\n",
        "    print(\"\\nTraining LDA model...\")\n",
        "    \n",
        "    n_topics = 8  # Start with 8 topics\n",
        "    \n",
        "    lda_model = LatentDirichletAllocation(\n",
        "        n_components=n_topics,\n",
        "        max_iter=20,\n",
        "        learning_method='online',\n",
        "        learning_offset=50.0,\n",
        "        random_state=42,\n",
        "        doc_topic_prior=None,  # Use default\n",
        "        topic_word_prior=None  # Use default\n",
        "    )\n",
        "    \n",
        "    # Fit LDA model\n",
        "    lda_fit = lda_model.fit(tfidf_matrix)\n",
        "    \n",
        "    # Get document-topic distributions\n",
        "    doc_topic_dist = lda_fit.transform(tfidf_matrix)\n",
        "    \n",
        "    print(f\"✅ LDA model trained with {n_topics} topics\")\n",
        "    print(f\"Model perplexity: {lda_fit.perplexity(tfidf_matrix):.2f}\")\n",
        "    print(f\"Log likelihood: {lda_fit.score(tfidf_matrix):.2f}\")\n",
        "    \n",
        "    # Step 3: Extract and Display Topics\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"LDA TOPICS DISCOVERED\")\n",
        "    print(\"=\"*80)\n",
        "    \n",
        "    def display_topics(model, feature_names, n_top_words=10):\n",
        "        topic_summaries = []\n",
        "        for topic_idx, topic in enumerate(model.components_):\n",
        "            top_words_idx = topic.argsort()[-n_top_words:][::-1]\n",
        "            top_words = [feature_names[i] for i in top_words_idx]\n",
        "            top_weights = [topic[i] for i in top_words_idx]\n",
        "            \n",
        "            print(f\"\\nTopic {topic_idx + 1}:\")\n",
        "            words_with_weights = [f\"{word}({weight:.3f})\" for word, weight in zip(top_words, top_weights)]\n",
        "            print(f\"  Top words: {', '.join(words_with_weights)}\")\n",
        "            \n",
        "            # Create a readable topic summary\n",
        "            topic_summary = ' + '.join([f\"{weight:.3f}*{word}\" for word, weight in zip(top_words[:5], top_weights[:5])])\n",
        "            topic_summaries.append(topic_summary)\n",
        "            \n",
        "        return topic_summaries\n",
        "    \n",
        "    lda_topic_summaries = display_topics(lda_fit, feature_names, n_top_words=10)\n",
        "    \n",
        "    # Step 4: Analyze Document-Topic Distributions\n",
        "    print(f\"\\n\" + \"=\"*60)\n",
        "    print(\"DOCUMENT-TOPIC ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Find dominant topics for sample documents\n",
        "    dominant_topics = np.argmax(doc_topic_dist, axis=1)\n",
        "    topic_counts = Counter(dominant_topics)\n",
        "    \n",
        "    print(f\"Topic distribution across documents:\")\n",
        "    for topic_id, count in sorted(topic_counts.items()):\n",
        "        percentage = (count / len(documents)) * 100\n",
        "        print(f\"  Topic {topic_id + 1}: {count} documents ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Show example documents for each topic\n",
        "    print(f\"\\nExample documents by topic:\")\n",
        "    for topic_id in range(min(n_topics, 3)):  # Show first 3 topics\n",
        "        topic_docs = [i for i, t in enumerate(dominant_topics) if t == topic_id]\n",
        "        if topic_docs:\n",
        "            doc_idx = topic_docs[0]\n",
        "            print(f\"\\nTopic {topic_id + 1} example:\")\n",
        "            print(f\"  Title: {df_sample.iloc[doc_idx]['title']}\")\n",
        "            print(f\"  Confidence: {doc_topic_dist[doc_idx][topic_id]:.3f}\")\n",
        "            print(f\"  Content: {df_sample.iloc[doc_idx]['description'][:200]}...\")\n",
        "    \n",
        "    print(\"✅ LDA topic modeling completed!\")\n",
        "    \n",
        "else:\n",
        "    print(\"❌ No preprocessed documents available for LDA modeling\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nlp_py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
